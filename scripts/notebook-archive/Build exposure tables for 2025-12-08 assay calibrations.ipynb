{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build exposure tables for MAVEs calibrated by Dan's methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports and constants\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from cvfgaou import hailtools, gctools, data\n",
    "from cvfgaou.notation import GEQ_CHAR, LEQ_CHAR\n",
    "\n",
    "BUCKET = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "\n",
    "#SPLICE_AI_FILTER_MAX = 0.2 # Splice AI filtering should happen at the cohort level, not at the variant level\n",
    "#AF_FILTER_MAX = 0.01 # We don't filter for rare variants in functional data\n",
    "CALIBRATION_VERSION = '2025-12-08'\n",
    "CALIBRATION_DIR = f'{BUCKET}/calibrations/calibrations_12_08_25'\n",
    "DATAFRAME_VERSION = '17796333'\n",
    "RESULTS_DIR = f'{BUCKET}/classes_2025-12-19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "hl.init()\n",
    "wgs_mt_path = os.getenv(\"WGS_EXOME_SPLIT_HAIL_PATH\")\n",
    "wgs_mt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wgs\n",
    "wgs_mt = hl.read_matrix_table(wgs_mt_path)\n",
    "wgs_mt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $WORKSPACE_BUCKET/cvfg_17796333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CVFG dataframe\n",
    "cvfg_df = pd.read_csv(\n",
    "    f'{BUCKET}/cvfg_17796333/final_pillar_data_with_clinvar_18_25_gnomad_wREVEL_wAM_wspliceAI_wMutpred2_wtrainvar_gold_standards_expanded_111225.csv.gz',\n",
    "    dtype={\n",
    "        'Dataset': str,\n",
    "        'Gene': str,\n",
    "        'Chrom': str,\n",
    "        #'hg38_start': int,\n",
    "        #'hg38_end': int,\n",
    "        'ref_allele': str,\n",
    "        'alt_allele': str,\n",
    "        #'auth_reported_score': float,\n",
    "        'auth_reported_func_class': str\n",
    "    }\n",
    ")\n",
    "cvfg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cvfg_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe:\n",
    "\n",
    "working_df = cvfg_df[\n",
    "    cvfg_df.Gene.isin(data.gene_phenotypes) & # Limit to our genes\n",
    "    (cvfg_df.Flag != '*') # Drop flagged variants (mapping errors etc.)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar_bins_df = pd.read_csv(f'{BUCKET}/clinvar/clinvar-bins.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $WORKSPACE_BUCKET/calibrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save exposure for each study in its own file for simplicity\n",
    "for blob in tqdm(gctools.list_blobs(CALIBRATION_DIR, return_uris=False)):\n",
    "    if blob.path.endswith('.json'):\n",
    "        calibration = json.loads(blob.download_as_text())\n",
    "        \n",
    "        dataset = calibration['dataset']\n",
    "        \n",
    "        exposures_file, clinvar_file, af_file = (\n",
    "            f'{RESULTS_DIR}/{result_type}/calibrated_{CALIBRATION_VERSION}_{dataset}.parquet'\n",
    "            for result_type in ('exposures', 'clinvar_maps', 'af_maps')\n",
    "        )\n",
    "        \n",
    "        if gctools.blob_exists(exposures_file):\n",
    "            print(f'Skipping {exposures_file} because it exists.')\n",
    "            continue\n",
    "            pass\n",
    "        \n",
    "        dataset_df = working_df[working_df['Dataset'] == dataset]\n",
    "        \n",
    "        if dataset_df.empty:\n",
    "            print(f'{dataset} not found in dataframe')\n",
    "            continue\n",
    "        \n",
    "        # Directionality of thresholds\n",
    "        increasing = calibration.get('scoreset_flipped') == True\n",
    "        if increasing:\n",
    "            print(f'{dataset} was labeled as increasing')\n",
    "        \n",
    "        # 2025-11-06 thresholds are provided as a dictionary of lists\n",
    "        thresholds_dict = {}\n",
    "        for point_str, value_range in calibration['point_ranges'].items():\n",
    "            \n",
    "            if not value_range: continue\n",
    "            \n",
    "            points = int(point_str)\n",
    "            \n",
    "            # In decreasing scoresets, the threshold is at the bottom of the interval and we include\n",
    "            # all strictly greater scoring variants in the class for benign evidence.\n",
    "            # We do the opposite for pathogenic evidence.\n",
    "            # We do the opposite of the above for increaseing scoresets.            \n",
    "            threshold_is_bottom = (points < 0) ^ increasing\n",
    "            \n",
    "            thresholds_dict[points] = {\n",
    "                'Classification': f'{GEQ_CHAR if points > 0 else LEQ_CHAR} {points:+d}',\n",
    "                'Threshold': value_range[0][0 if threshold_is_bottom else 1],\n",
    "                'Comparison': pd.Series.ge if threshold_is_bottom else pd.Series.le\n",
    "            }\n",
    "        \n",
    "        # Can't work without thresholds\n",
    "        if not thresholds_dict:\n",
    "            print(f'No thresholds for {dataset}.')\n",
    "            continue\n",
    "        \n",
    "        thresholds_df = pd.DataFrame.from_dict(thresholds_dict, orient='index').sort_index(ascending=True)\n",
    "        \n",
    "        # Check that threholds are increasing(decreasing):\n",
    "        if increasing:\n",
    "            assert thresholds_df['Threshold'].is_monotonic_increasing, f'Thresholds for {dataset} are expected to be increasing: {thresholds_df}'\n",
    "        else:\n",
    "            assert thresholds_df['Threshold'].is_monotonic_decreasing, f'Thresholds for {dataset} are expected to be decreasing: {thresholds_df}'\n",
    "                \n",
    "        gene_result_dfs = []\n",
    "        clinvar_classes_dfs = []\n",
    "        joint_af_map = {}\n",
    "        \n",
    "        for classification, threshold, compare in tqdm(thresholds_df.itertuples(index=False)):\n",
    "            \n",
    "            for gene, gene_df in dataset_df.groupby('Gene'):\n",
    "                \n",
    "                variant_df = gene_df[\n",
    "                    ['Chrom', 'hg38_start', 'ref_allele', 'alt_allele']\n",
    "                ][\n",
    "                    compare(gene_df['auth_reported_score'].astype(float), threshold)\n",
    "                ].dropna(how='any').assign(Chromosome = lambda df: 'chr'+df['Chrom']).astype({'hg38_start': int})\n",
    "            \n",
    "                if variant_df.empty:\n",
    "                    continue\n",
    "\n",
    "                exposure_df, af_map, clinvar_df = hailtools.get_exposure_package(\n",
    "                    variant_df,\n",
    "                    wgs_mt,\n",
    "                    clinvar_bins_df,\n",
    "                    contig_col='Chromosome',\n",
    "                    pos_col='hg38_start',\n",
    "                    ref_col='ref_allele',\n",
    "                    alt_col='alt_allele',\n",
    "                    metadata_dict={\n",
    "                        'Dataset': dataset,\n",
    "                        'Gene': gene,\n",
    "                        'Classifier': f'Calibrated ({CALIBRATION_VERSION})',\n",
    "                        'Classification': classification,\n",
    "                        'Data Version': DATAFRAME_VERSION\n",
    "                    } | ({} if (gene_df.splice_measure == 'Yes').all() else {\n",
    "                        'SpliceAI filter max': SPLICE_AI_FILTER_MAX\n",
    "                    })\n",
    "                )\n",
    "                \n",
    "                clinvar_classes_dfs.append(clinvar_df)\n",
    "                joint_af_map.update(af_map)\n",
    "                gene_result_dfs.append(exposure_df)\n",
    "            \n",
    "        if clinvar_classes_dfs:\n",
    "            pd.concat(clinvar_classes_dfs, ignore_index=True).to_parquet(clinvar_file, index=False)\n",
    "        if joint_af_map:\n",
    "            pd.Series(joint_af_map).to_frame(name='AF').to_parquet(af_file)\n",
    "        if gene_result_dfs:\n",
    "            pd.concat(gene_result_dfs, ignore_index=True).to_parquet(exposures_file, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $WORKSPACE_BUCKET/calibrations/calibrations_11_06_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat $WORKSPACE_BUCKET/calibrations/calibrations_11_06_25/TP53_Boettcher_2019.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
