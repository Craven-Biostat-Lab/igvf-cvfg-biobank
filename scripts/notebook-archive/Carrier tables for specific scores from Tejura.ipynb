{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the table provided by Malvika Tejura to extract specific variant classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports and constants\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from cvfgaou import hailtools, gctools, notation, data\n",
    "from cvfgaou.notation import GEQ_CHAR, LEQ_CHAR\n",
    "\n",
    "BUCKET = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "\n",
    "DATAFRAME_VERSION = '17796333' # For bookkeeping\n",
    "RESULTS_DIR = f'{BUCKET}/classes_2025-12-19'\n",
    "OVERWRITE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load points dataframe\n",
    "\n",
    "points_df = pd.read_csv(\n",
    "    f'{BUCKET}/precomputed/Dan_fxn_calibrations_new_2025_2018_clust_calib_121425.csv.gz',\n",
    "    dtype = {\n",
    "        'Gene': str,\n",
    "        'Dataset': str,\n",
    "        'consequence': str,\n",
    "        'Chrom': str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to our genes, and remove flagged variants\n",
    "working_df = points_df[\n",
    "    points_df['Gene'].isin(data.gene_phenotypes) &\n",
    "    (points_df.Flag != '*')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sets = [\n",
    "    {\n",
    "        'Dataset': 'TP53_Fayer_2021_meta',\n",
    "        'Col': 'OP_points_18_25',\n",
    "        'Points': True\n",
    "    }\n",
    "] + [\n",
    "    {\n",
    "        'Dataset': dataset,\n",
    "        'Col': 'StandardizedClass',\n",
    "        'Points': False\n",
    "    }\n",
    "    for dataset in working_df.Dataset.drop_duplicates()\n",
    "] + [\n",
    "    {\n",
    "        'Dataset': dataset,\n",
    "        'Col': 'StandardizedClass',\n",
    "        'Points': False,\n",
    "        stop_gain_flag: True\n",
    "    }\n",
    "    for dataset in working_df.Dataset[working_df.Dataset.str.endswith('_unpublished')].drop_duplicates()\n",
    "    for stop_gain_flag in ('Stop gain only', 'All but stop gain')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "hl.init()\n",
    "wgs_mt_path = os.getenv(\"WGS_EXOME_SPLIT_HAIL_PATH\")\n",
    "wgs_mt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wgs\n",
    "wgs_mt = hl.read_matrix_table(wgs_mt_path)\n",
    "wgs_mt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer points ranges\n",
    "\n",
    "point_groups = {\n",
    "    '0' if points == 0 else f'{GEQ_CHAR if points > 0 else LEQ_CHAR} {points:+d}':\n",
    "        (pd.Series.eq, points) if points == 0 else (\n",
    "            (pd.Series.ge, points) if points > 0 else (pd.Series.le, points)\n",
    "        )\n",
    "#        ((lambda x: dummy_geq(x, points)) if points > 0 else (lambda x: dummy_leq(x, points)))\n",
    "#        ((lambda x: x >= points) if points > 0 else (lambda x: x <= points))\n",
    "    for points in range(-16, 17)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar_bins_df = pd.read_csv(f'{BUCKET}/clinvar/clinvar-bins.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (gene, dataset), dataset_df in tqdm(working_df.groupby(['Gene', 'Dataset'])):\n",
    "\n",
    "    exposures_file = f'{RESULTS_DIR}/exposures/{dataset}.parquet'\n",
    "    clinvar_file = f'{RESULTS_DIR}/clinvar_maps/{dataset}.parquet'\n",
    "    af_file = f'{RESULTS_DIR}/af_maps/{dataset}.parquet'\n",
    "    \n",
    "    if all((gctools.blob_exists(f) for f in (exposures_file, clinvar_file, af_file))) and not OVERWRITE:\n",
    "        print(f'Files for {dataset} exist, skipping.')\n",
    "        continue\n",
    "\n",
    "    clinvar_classes_dfs = []\n",
    "    joint_af_map = {}\n",
    "    gene_result_dfs = []\n",
    "    \n",
    "    # Select relevant score sets.\n",
    "    \n",
    "    relevant_scoresets = [\n",
    "        score_set for score_set in score_sets if score_set['Dataset'] == dataset\n",
    "    ]\n",
    "    \n",
    "    for score_set in tqdm(relevant_scoresets):\n",
    "        \n",
    "        classifier = score_set['Col']        \n",
    "        score_set_df = dataset_df.copy()\n",
    "        \n",
    "        if score_set_df.empty:\n",
    "            continue\n",
    "        \n",
    "        if score_set.get('Stop gain only'):\n",
    "            classifier += ' (stop gain only)'\n",
    "            score_set_df = score_set_df[\n",
    "                score_set_df['consequence'].str.contains('stop_gained', na = False)\n",
    "            ]\n",
    "        \n",
    "        if score_set.get('All but stop gain'):\n",
    "            classifier += ' (without stop gain)'\n",
    "            score_set_df = score_set_df[\n",
    "                score_set_df['consequence'].isna() |\n",
    "                ~score_set_df['consequence'].str.contains('stop_gained', na = False)\n",
    "            ]\n",
    "        \n",
    "        score_set_df = score_set_df[\n",
    "            ['Chrom', 'hg38_start', 'ref_allele', 'alt_allele', score_set['Col']]\n",
    "        ].rename(\n",
    "            columns={score_set['Col']: 'score'}\n",
    "        ).dropna().astype({\n",
    "            'Chrom': str,\n",
    "            'hg38_start': int,\n",
    "            'ref_allele': str,\n",
    "            'alt_allele': str,\n",
    "            'score': float if score_set['Points'] else str\n",
    "        }).assign(Chromosome = lambda df: 'chr' + df[\"Chrom\"])\n",
    "\n",
    "        if score_set['Points']:\n",
    "            classification_dfs = {\n",
    "                classification:\n",
    "                    score_set_df[\n",
    "                        ['Chromosome', 'hg38_start', 'ref_allele', 'alt_allele']\n",
    "                    ][\n",
    "                        compare(score_set_df.score, threshold)\n",
    "                    ]\n",
    "                for classification, (compare, threshold) in point_groups.items()\n",
    "            }\n",
    "        else:\n",
    "            classification_dfs = {\n",
    "                classification:\n",
    "                    score_set_df[\n",
    "                        ['Chromosome', 'hg38_start', 'ref_allele', 'alt_allele']\n",
    "                    ][\n",
    "                        score_set_df.score == classification\n",
    "                    ]\n",
    "                for classification in ('NORMAL', 'ABNORMAL')\n",
    "            }\n",
    "        \n",
    "        for classification, variant_df in classification_dfs.items():\n",
    "\n",
    "            if variant_df.empty:\n",
    "                continue\n",
    "\n",
    "            exposure_df, af_map, clinvar_df = hailtools.get_exposure_package(\n",
    "                variant_df,\n",
    "                wgs_mt,\n",
    "                clinvar_bins_df,\n",
    "                contig_col='Chromosome',\n",
    "                pos_col='hg38_start',\n",
    "                ref_col='ref_allele',\n",
    "                alt_col='alt_allele',\n",
    "                metadata_dict={\n",
    "                    'Dataset': dataset,\n",
    "                    'Gene': gene,\n",
    "                    'Classifier': classifier,\n",
    "                    'Classification': classification,\n",
    "                    'Data Version': DATAFRAME_VERSION\n",
    "                }\n",
    "            )\n",
    "\n",
    "            clinvar_classes_dfs.append(clinvar_df)\n",
    "            joint_af_map.update(af_map)\n",
    "            gene_result_dfs.append(exposure_df)\n",
    "\n",
    "    if clinvar_classes_dfs:\n",
    "        pd.concat(clinvar_classes_dfs, ignore_index=True).to_parquet(clinvar_file)\n",
    "    if joint_af_map:\n",
    "        pd.Series(joint_af_map).to_frame(name='AF').to_parquet(af_file)\n",
    "    if gene_result_dfs:\n",
    "        pd.concat(gene_result_dfs, ignore_index=True).to_parquet(exposures_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
